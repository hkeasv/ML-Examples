{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chapter 2 â€“ End-to-end Machine Learning project**\n",
    "\n",
    "# Contents\n",
    "1. Get the data\n",
    "2. Data analysis\n",
    "3. Data preparation\n",
    "4. Select, train and fine-tune a model\n",
    "5. Evaluate the models on the test set\n",
    "\n",
    "\n",
    "Your task is to predict median house values in Californian districts, given a number of features from these districts. A district typically has a population of 600 to 3000 people.\n",
    "\n",
    "This is a regression task, since you are asked to predict a numerical value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import sklearn\n",
    "import numpy as np\n",
    "\n",
    "# Import the SSL module and configure certificate verification to be optional to avoid SSLCertVerificationError.\n",
    "import ssl\n",
    "ssl.SSLContext.verify_mode = ssl.VerifyMode.CERT_OPTIONAL\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates a dataset/housing directory subdirectory, if it doesn't already exists. Then it downloads \n",
    "# the housing.tgz file, and extracts the housing.csv file.\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "def load_housing_data():\n",
    "    tarball_path = Path(\"datasets/housing.tgz\")\n",
    "    if not tarball_path.is_file():\n",
    "        Path(\"datasets\").mkdir(parents=True, exist_ok=True)\n",
    "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\"\n",
    "        urllib.request.urlretrieve(url, tarball_path)\n",
    "        with tarfile.open(tarball_path) as housing_tarball:\n",
    "            housing_tarball.extractall(path=\"datasets\")\n",
    "    return pd.read_csv(Path(\"datasets/housing/housing.csv\"))\n",
    "\n",
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the top five rows\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a quick description of the data\n",
    "# (look for null values and non-numerical data which require special data preparation)\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the non-numerical feature, ocean_proximity\n",
    "housing[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics for the features\n",
    "housing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot a histogram of each numerical feature\n",
    "import matplotlib.pyplot as plt\n",
    "housing.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations in the histograms\n",
    "1) housing_median_age, median_house_value and median_income have been capped. If this is a problem, we could\n",
    "   remove the largest values (we will omit that here for simplicity)\n",
    "2) median_income has been scaled to tens of thousand dollars.\n",
    "3) Some histograms are tail-heavy (may be a problem for some algorithms). Although a feature can be made less\n",
    "   tail-heavy by using simple techniques, such as replacing each value with its square root or the logarithm,\n",
    "   we will omit that here for simplicity.\n",
    "4) The features has different scales (scaling may be required)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a geographical scatterplot of the data\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", grid=True, alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A better visualization. The radius of each circle (option s) represents the district's population, and the\n",
    "# color (option c) represents the price.\n",
    "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\",\n",
    "    s=housing[\"population\"]/100, label=\"population\",\n",
    "    c=\"median_house_value\", cmap=\"jet\", colorbar=True,\n",
    "    legend=True, sharex=False, figsize=(10,7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations\n",
    "The plot above shows that the prices are much related to the location and the population density.\n",
    "\n",
    "Let us now compute and illustrate linear correlations between selected attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much does each feature correlate with the median house value?\n",
    "# The correlation coefficient ranges from -1 (100% negative correlation) to 1 (100% positive correlation)\n",
    "corr_matrix = housing.corr(numeric_only=True)\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute combinations\n",
    "The total number of rooms in a district is not very useful. Instead, we want the number of rooms per household. Similarly, we want the number of bedrooms per room instead of the total number of bedrooms in a district, and the population per household instead of the total population in a district."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
    "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"]\n",
    "housing[\"population_per_household\"]=housing[\"population\"]/housing[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much does each of the new attributes correlate with the median house value?\n",
    "corr_matrix = housing.corr(numeric_only=True)\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will keep the new attributes and delete the original ones.\n",
    "housing = housing.drop(columns=\"total_rooms\")\n",
    "housing = housing.drop(columns=\"total_bedrooms\")\n",
    "housing = housing.drop(columns=\"population\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "<ol>\n",
    "<li>how to split the dataset in a training set and a test set</li>\n",
    "<li>how to clean the data so that there are no missing values</li>\n",
    "<li>how to re-scale attribute values</li>\n",
    "<li>and how to handle a non-numerical feature</li>\n",
    "</ol>\n",
    "\n",
    "## Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset randomly in training set (80%) and test set (20%). Use a fixed random seed (42).\n",
    "# As a rule of thumb, pick 20% for the test set, unless the dataset is very large.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stratified sampling\n",
    "Random sampling is fine, if the dataset is large enough. If not, stratified sampling should be considered. Stratified sampling ensures that the test set is representative of the whole dataset. The dataset is first divided into subgroups called strata (e.g. a stratum could represent an income group). Then the right number of instances for both training and test sets are picked from each stratum. Stratified sampling reduces sampling bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the labels from the training set (the method returns a new set, and it does not affect the original one).\n",
    "housing_predictors = train_set.drop(columns=\"median_house_value\")\n",
    "# Keep the labels in a separate set.\n",
    "housing_labels = train_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "\n",
    "Strategies:\n",
    "<ol>\n",
    "<li>Remove all rows with missing values. Use the DataFrame's dropna() method.</li>\n",
    "<li>Remove all columns with missing values.</li>\n",
    "<li>Replace missing values with some default value (mean, median, most frequent or some fixed value). Use Scikit-Learn's SimpleImputer class.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the text attribute because median can only be calculated on numerical attributes.\n",
    "housing_num = housing_predictors.select_dtypes(include=[np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the median of each attribute.\n",
    "imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The result is stored in the statistics_ instance variable.\n",
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replace missing values with the medians.\n",
    "imputer.transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-Learn transformers returns NumPy arrays even when they get a Pandas DataFrame as input.\n",
    "housing_num_numpy = imputer.transform(housing_num)\n",
    "\n",
    "# If you want, you can wrap the returned NumPy array in a DataFrame, with the following code:\n",
    "housing_dataframe = pd.DataFrame(housing_num_numpy, columns=housing_num.columns, index=housing_num.index)\n",
    "housing_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also convert a Pandas DataFrame to a numpy array with the following code:\n",
    "housing_dataframe.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling\n",
    "Most ML algorithms don't perform well, when the numerical input attributes have different scales. They tend to focus more on features with a large range of values, and less on features with a small range of values. There are two common  ways of scaling:\n",
    "\n",
    "Min-max scaling (also called normalization): values are rescaled so that they end up ranging from 0 to 1.\n",
    "Standardization: values are rescaled so that they have zero mean and unit variance.\n",
    "\n",
    "Standardization is much less affected by outliers, but the lack of a fixed range (0 to 1) is a problem for some algorithms (e.g. neural networks).\n",
    "\n",
    "Scaling the target values is generally not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()\n",
    "housing_num_scaled = scaler.fit_transform(housing_num)\n",
    "housing_num_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling non-numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the single non-numerical attribute, which is categorial.\n",
    "housing_cat = housing_predictors[[\"ocean_proximity\"]]\n",
    "\n",
    "# Show the different categories.\n",
    "housing_cat.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one binary attribute per category.\n",
    "# It is called one-hot encoding, because only one attribute value will be 1 (hot) while the others will be 0 (cold).\n",
    "# The default output is a sparse matrix, which only stores the location of the non-zero elements. Here we will use\n",
    "# a dense matrix (sparse=False), since there are only a few categories.\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "cat_encoder = OneHotEncoder(sparse_output=False)\n",
    "housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n",
    "housing_cat_1hot\n",
    "\n",
    "# Other ways of converting a categorical attribute:\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original categories are stored in the categories_ instance variable.\n",
    "cat_encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation pipelines\n",
    "The Scikit-Learn Pipeline class can perform a sequence of transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for the numerical attributes.\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "num_pipeline = make_pipeline(SimpleImputer(strategy=\"median\"), MinMaxScaler())\n",
    "\n",
    "housing_num_transformed = num_pipeline.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for the categorical attribute.\n",
    "cat_pipeline = make_pipeline(SimpleImputer(strategy=\"most_frequent\"), OneHotEncoder(sparse_output=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline that will transform both the numerical and categorial attributes and combine them.\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# We must pass the names of the attributes which should be transformed\n",
    "num_attribs = list(housing_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "preprocessing_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", cat_pipeline, cat_attribs),\n",
    "    ])\n",
    "\n",
    "# Alternative construct:\n",
    "#from sklearn.compose import make_column_selector, make_column_transformer\n",
    "\n",
    "#preprocessing_pipeline = make_column_transformer(\n",
    "#    (num_pipeline, make_column_selector(dtype_include=np.number)),\n",
    "#    (cat_pipeline, make_column_selector(dtype_include=object)),\n",
    "#)\n",
    "\n",
    "housing_predictors_prepared = preprocessing_pipeline.fit_transform(housing_predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_predictors_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method outputs the number of rows and columns in the dataset.\n",
    "housing_predictors_prepared.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select, train and fine-tune a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DecisionTreeRegressor and add it to the preprocessing pipeline.\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "tree_reg = Pipeline([\n",
    "    (\"preprocessing\", preprocessing_pipeline),\n",
    "    (\"decision_tree\", DecisionTreeRegressor(random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GridSearchCV to fine-tune hyperparameters for a DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'decision_tree__max_depth': list(range(10, 30)),'decision_tree__max_leaf_nodes': [400, 500, 600], \n",
    "          'decision_tree__min_samples_split': [40, 60, 80]}\n",
    "\n",
    "grid_search = GridSearchCV(tree_reg, params, n_jobs=-1, cv=5)\n",
    "\n",
    "# Train (beware that the preprocessing pipeline will run before each decision tree is trained)\n",
    "grid_search.fit(housing_predictors, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display parameters for the best estimator\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the models RMSE on the training set\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# We input the unprepared predictors of the training set to the predict method, because the predict method\n",
    "# will run the preprocessing_pipeline on the input data, before making predictions.\n",
    "housing_predictions = grid_search.predict(housing_predictors)\n",
    "grid_search_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False)\n",
    "grid_search_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the labels from the test set.\n",
    "X_test = test_set.drop(\"median_house_value\", axis=1)\n",
    "# Keep the labels in a separate set.\n",
    "y_test = test_set[\"median_house_value\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the DecisionTreeRegressor.\n",
    "# Remember that the predict method will run the preprocessing_pipeline on X_test, before making predictions.\n",
    "grid_search_predictions = grid_search.predict(X_test)\n",
    "grid_search_rmse = mean_squared_error(y_test, grid_search_predictions, squared=False)\n",
    "grid_search_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
